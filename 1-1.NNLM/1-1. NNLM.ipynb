{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch():\n",
    "    input_batch = []  # 마지막 단어 이전 단어까지의 word_dict value\n",
    "    target_batch = [] # 마지막 단어의 word_dict value\n",
    "\n",
    "    for sen in sentences:\n",
    "        word = sen.split() # space tokenizer\n",
    "        input = [word_dict[n] for n in word[:-1]] # create (1~n-1) as input\n",
    "        target = word_dict[word[-1]] # create (n) as target, We usually call this 'casual language model'\n",
    "\n",
    "        input_batch.append(input)\n",
    "        target_batch.append(target)\n",
    "\n",
    "    return input_batch, target_batch\n",
    "# input_batch: [[6, 2], [6, 0], [6, 4]]\n",
    "# target_batch: [5, 1, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class NNLM(nn.Module): # nn.Module을 상속 받는다\n",
    "    def __init__(self):\n",
    "        super(NNLM, self).__init__() # nn.Module의 생성자 함수 호출\n",
    "        self.C = nn.Embedding(n_class, m) # n_class: embedding의 딕셔너리 크기, m: embedding 벡터의 크기\n",
    "        self.H = nn.Linear(n_step * m, n_hidden, bias=False) # n_step*m: 각 input의 크기, n_hidden: 각 output의 크기\n",
    "        self.d = nn.Parameter(torch.ones(n_hidden)) # 파라미터로 넣어줄 tensor\n",
    "        self.U = nn.Linear(n_hidden, n_class, bias=False) # n_hidden: 각 input의 크기, n_class: 각 output의 크기 \n",
    "        self.W = nn.Linear(n_step * m, n_class, bias=False)\n",
    "        self.b = nn.Parameter(torch.ones(n_class))\n",
    "\n",
    "    def forward(self, X):\n",
    "#         X: tensor([[6, 2], \n",
    "#                    [6, 0], \n",
    "#                    [6, 4]])\n",
    "\n",
    "        X = self.C(X) # X : [batch_size, n_step, m]\n",
    "\n",
    "#         X: tensor([[[-0.3198,  0.3602],\n",
    "#                    [ 0.3201, -0.7619]],\n",
    "#                   [[-0.3198,  0.3602],\n",
    "#                    [-2.1598, -0.1377]],\n",
    "#                   [[-0.3198,  0.3602],\n",
    "#                    [ 1.1099,  1.4730]]], grad_fn=<EmbeddingBackward>)\n",
    "\n",
    "        X = X.view(-1, n_step * m) # [batch_size, n_step * m]\n",
    "        tanh = torch.tanh(self.d + self.H(X)) # [batch_size, n_hidden]\n",
    "        output = self.b + self.W(X) + self.U(tanh) # [batch_size, n_class]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3247, -1.0386,  1.7355,  0.7373,  2.5494,  2.0638,  0.6364],\n",
       "        [ 0.5098, -1.2549,  2.0247,  0.6980,  2.6067,  1.8777,  0.6331],\n",
       "        [-0.3549,  0.3776,  0.4636,  0.6714,  1.6076,  1.6843,  0.4200]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_step = 2 # number of steps, n-1 in paper\n",
    "n_hidden = 2 # number of hidden size, h in paper\n",
    "m = 2 # size of embedding vector, m in paper\n",
    "\n",
    "sentences = [\"i like dog\", \"i love coffee\", \"i hate milk\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'like', 'dog', 'i', 'love', 'coffee', 'i', 'hate', 'milk']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list = \" \".join(sentences).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['love', 'coffee', 'like', 'milk', 'hate', 'dog', 'i']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list = list(set(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'love': 0, 'coffee': 1, 'like': 2, 'milk': 3, 'hate': 4, 'dog': 5, 'i': 6}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict = {w: i for i, w in enumerate(word_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'love', 1: 'coffee', 2: 'like', 3: 'milk', 4: 'hate', 5: 'dog', 6: 'i'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_dict = {i: w for i, w in enumerate(word_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_class = len(word_dict)  # number of Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NNLM()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) \n",
    "# Adam()의 첫 번째 인자에는 파라미터를 넣어준다\n",
    "# parameters(): 모델의 모든 하위 모듈을 탐색하면서 모든 파라미터를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch, target_batch = make_batch()\n",
    "# input_batch: [[6, 2], [6, 0], [6, 4]]\n",
    "# target_batch: [5, 1, 3]\n",
    "\n",
    "input_batch = torch.LongTensor(input_batch) # LongTensor(): 64비트의 부호 있는 정수\n",
    "target_batch = torch.LongTensor(target_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000 cost = 0.046850\n",
      "Epoch: 2000 cost = 0.009255\n",
      "Epoch: 3000 cost = 0.003518\n",
      "Epoch: 4000 cost = 0.001652\n",
      "Epoch: 5000 cost = 0.000854\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for epoch in range(5000):\n",
    "    optimizer.zero_grad() # backward()를 할 때 gradient를 계속 더해주기 때문에 항상 zero로 만들어주고 시작해야함\n",
    "    output = model(input_batch)\n",
    "\n",
    "    # output : [batch_size, n_class], target_batch : [batch_size]\n",
    "    loss = criterion(output, target_batch) # loss 계산\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step() # optimization 단계를 한번 수행한다. 파라미터를 업데이트한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5],\n",
       "        [1],\n",
       "        [3]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict\n",
    "predict = model(input_batch).data.max(1, keepdim=True)[1]\n",
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'like'], ['i', 'love'], ['i', 'hate']] -> ['dog', 'coffee', 'milk']\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "print([sen.split()[:2] for sen in sentences], '->', [number_dict[n.item()] for n in predict.squeeze()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
