{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a2943b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ae634b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S: decoder input의 시작을 나타냄\n",
    "# E: decoder output의 끝을 나타냄\n",
    "# P: 현재 batch의 단어 길이가 n_step보다 작은 경우 빈 곳을 'P'로 채움\n",
    "\n",
    "def make_batch():\n",
    "    input_batch, output_batch, target_batch = [], [], []\n",
    "    \n",
    "    onehot_vector = np.eye(n_class)\n",
    "    \n",
    "    for seq in seq_data:\n",
    "        for i in range(2):\n",
    "            seq[i] = seq[i] + 'P' * (n_step - len(seq[i])) # 빈 곳 채우기\n",
    "        \n",
    "        input = [ch_to_idx[ch] for ch in seq[0]]\n",
    "        output = [ch_to_idx[ch] for ch in ('S' + seq[1])] # decoder의 input\n",
    "        target = [ch_to_idx[ch] for ch in (seq[1] + 'E')]\n",
    "        \n",
    "        input_batch.append(onehot_vector[input])\n",
    "        output_batch.append(onehot_vector[output])\n",
    "        target_batch.append(target)\n",
    "        \n",
    "    # Tensor로 변환\n",
    "    input_batch = torch.FloatTensor(input_batch)\n",
    "    output_batch = torch.FloatTensor(output_batch)\n",
    "    target_batch = torch.LongTensor(target_batch)\n",
    "        \n",
    "    return input_batch, output_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e1050a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_testbatch(input_word):\n",
    "    input_batch, output_batch = [], []\n",
    "    \n",
    "    onehot_vector = np.eye(n_class)\n",
    "    \n",
    "    input_word = input_word + 'P' * (n_step - len(input_word))\n",
    "    input = [ch_to_idx[ch] for ch in input_word]\n",
    "    output_word = 'S' + 'P' * n_step\n",
    "    output = [ch_to_idx[ch] for ch in output_word]\n",
    "    \n",
    "    input_batch.append(onehot_vector[input])\n",
    "    output_batch.append(onehot_vector[output])\n",
    "    \n",
    "    input_batch = torch.FloatTensor(input_batch)\n",
    "    output_batch = torch.FloatTensor(output_batch)\n",
    "    \n",
    "    return input_batch, output_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "50700f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \n",
    "        self.enc_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
    "        self.dec_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
    "        self.fc = nn.Linear(n_hidden, n_class)\n",
    "        \n",
    "    def forward(self, enc_input, enc_hidden, dec_input):\n",
    "        enc_input = enc_input.transpose(0, 1) # (n_step, batch_size, n_class)\n",
    "        dec_input = dec_input.transpose(0, 1) # (n_step, batch_size, n_class)\n",
    "        \n",
    "        _, enc_states = self.enc_cell(enc_input, enc_hidden) # enc_states: (num_layers(=1)*num_directions(=1), batch_size, n_hidden), hidden state를 의미\n",
    "        outputs, _ = self.dec_cell(dec_input, enc_states)   # outputs: (n_step+1, batch_size, n_class)\n",
    "\n",
    "        result = self.fc(outputs) # (n_step+1, batch_size, n_class)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "04d1244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_list = [ch for ch in 'SEPabcdefghijklmnopqrstuvwxyz']\n",
    "ch_to_idx = {ch: idx for idx, ch in enumerate(ch_list)}\n",
    "idx_to_ch = {idx: ch for idx, ch in enumerate(ch_list)}\n",
    "\n",
    "seq_data = [['man', 'woman'], ['black', 'white'], ['king', 'queen'], ['girl', 'boy'], ['up', 'down'], ['high', 'low']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "12b520bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_step = 5\n",
    "n_hidden = 128\n",
    "\n",
    "n_class = len(ch_list)\n",
    "batch_size = len(seq_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5a9a9777",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\beaus\\python\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "model = Seq2Seq()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2b43e96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch, output_batch, target_batch = make_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c633a67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000  loss: 0.003422\n",
      "Epoch: 2000  loss: 0.000942\n",
      "Epoch: 3000  loss: 0.000402\n",
      "Epoch: 4000  loss: 0.000202\n",
      "Epoch: 5000  loss: 0.000109\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for epoch in range(5000):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    hidden = torch.zeros(1, batch_size, n_hidden)\n",
    "    \n",
    "    output = model(input_batch, hidden, output_batch)\n",
    "    output = output.transpose(0, 1) # (batch_size, n_step+1, n_class)\n",
    "    \n",
    "    loss = 0\n",
    "    for i in range(len(output)):\n",
    "        loss += criterion(output[i], target_batch[i])\n",
    "    \n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(\"Epoch: {:4d}  loss: {:.6f}\".format(epoch + 1, loss))\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "139b64e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "def test(word):\n",
    "    input_batch, output_batch = make_testbatch(word)\n",
    "    \n",
    "    hidden = torch.zeros(1*1, 1, n_hidden) # (num_layers*num_directions, batch_size, n_hidden)\n",
    "    output = model(input_batch, hidden, output_batch)\n",
    "    \n",
    "    predicts = output.data.max(2, keepdim=True)[1]\n",
    "    predicts_decoded = [idx_to_ch[predict.item()] for predict in predicts]\n",
    "    end_idx = predicts_decoded.index('E')\n",
    "    result = \"\".join(predicts_decoded[:end_idx]).replace('P', '')\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a04af1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== test =========\n",
      "man -> woman\n",
      "black -> white\n",
      "king -> queen\n",
      "girl -> boy\n",
      "up -> down\n",
      "high -> low\n"
     ]
    }
   ],
   "source": [
    "print('======== test =========')\n",
    "\n",
    "for seq in seq_data:\n",
    "    word = seq[0]\n",
    "    print(word, '->', test(word))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
