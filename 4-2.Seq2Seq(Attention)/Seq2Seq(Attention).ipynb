{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "50071ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "82d0c1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch():\n",
    "    onehot_vector = np.eye(n_class)\n",
    "    \n",
    "    input_batch = [onehot_vector[[word_to_idx[word] for word in sentences[0].split()]]]\n",
    "    output_batch = [onehot_vector[[word_to_idx[word] for word in sentences[1].split()]]]\n",
    "    target_batch = [[word_to_idx[word] for word in sentences[2].split()]]\n",
    "    \n",
    "    input_batch = torch.FloatTensor(input_batch)\n",
    "    output_batch = torch.FloatTensor(output_batch)\n",
    "    target_batch = torch.LongTensor(target_batch)\n",
    "    \n",
    "    return input_batch, output_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b13f5b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.enc_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
    "        self.dec_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
    "        \n",
    "        # Linear for Attention\n",
    "        self.attn = nn.Linear(n_hidden, n_hidden)\n",
    "        self.out = nn.Linear(n_hidden * 2, n_class)\n",
    "        \n",
    "        \n",
    "    def forward(self, enc_inputs, hidden, dec_inputs):\n",
    "        enc_inputs = enc_inputs.transpose(0, 1) # (n_step, batch_size, n_class)\n",
    "        dec_inputs = dec_inputs.transpose(0, 1) # (n_step, batch_size, n_class)\n",
    "        \n",
    "        enc_outputs, hidden = self.enc_cell(enc_inputs, hidden)\n",
    "        # enc_outputs: (n_step, batch_size, n_hidden)\n",
    "        # hidden: (1*1, batch_size, n_hidden)\n",
    "        \n",
    "        trained_attn = []\n",
    "        n_step = len(dec_inputs)\n",
    "        result = torch.empty([n_step, 1, n_class])\n",
    "        \n",
    "        for i in range(n_step):\n",
    "            dec_output, hidden = self.dec_cell(dec_inputs[i].unsqueeze(0), hidden)\n",
    "            # dec_output: (1, batch_size, n_hidden)\n",
    "            # hidden: (1*1, batch_size, n_hidden)\n",
    "            \n",
    "            attn_weights = self.get_attn_weight(dec_output, enc_outputs) # (1, 1, n_step)\n",
    "            trained_attn.append(attn_weights.squeeze().data.numpy())\n",
    "            \n",
    "            # bmm = batch matrix multiplication (matmul과 비슷)\n",
    "            context_vector = attn_weights.bmm(enc_outputs.transpose(0, 1))\n",
    "            # (1, 1, n_step) x (batch_size(=1), n_step, n_hidden) = (1, 1, n_hidden)\n",
    "            \n",
    "            dec_output = dec_output.squeeze(0) # (batch_size(=1), n_hidden)\n",
    "            context_vector = context_vector.squeeze(1) # (1, n_hidden)\n",
    "            \n",
    "            concat = torch.cat((dec_output, context_vector), 1) # (1, n_hidden * 2)\n",
    "            result[i] = self.out(concat) # (1, n_class)\n",
    "            \n",
    "        return result.transpose(0, 1).squeeze(0), trained_attn # (n_step, n_class), (1, n_step, n_step)\n",
    "            \n",
    "        \n",
    "    def get_attn_weight(self, dec_output, enc_outputs):\n",
    "        # get attention weight of one 'dec_output' with 'enc_outputs'\n",
    "        n_step = len(enc_outputs)\n",
    "        attn_scores = torch.zeros(n_step)\n",
    "        \n",
    "        for i in range(n_step):\n",
    "            attn_scores[i] = self.get_attn_score(dec_output, enc_outputs[i])\n",
    "        \n",
    "        return F.softmax(attn_scores, dim=0).view(1, 1, -1) # (1, 1, n_step)\n",
    "        \n",
    "            \n",
    "    def get_attn_score(self, dec_output, enc_output):\n",
    "        # dec_output: (1, batch_size, n_hidden)\n",
    "        # enc_output: (batch_size, n_hidden)\n",
    "        score = self.attn(enc_output) # (batch_size, n_hidden)\n",
    "        return torch.dot(dec_output.view(-1), score.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "faed288c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_step = 5\n",
    "n_hidden = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1b5f494c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"ich mochte ein bier P\", \"S i want a beer\", \"i want a beer E\"]\n",
    "\n",
    "word_list = list(set(\" \".join(sentences).split()))\n",
    "word_to_idx = {word: idx for idx, word in enumerate(word_list)}\n",
    "idx_to_word = {idx: word for idx, word in enumerate(word_list)}\n",
    "n_class = len(word_list)\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1f61e8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Attention()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d9f3b89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch, output_batch, target_batch = make_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b36a43e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  100  loss: 0.003183\n",
      "Epoch :  200  loss: 0.001320\n",
      "Epoch :  300  loss: 0.000731\n",
      "Epoch :  400  loss: 0.000468\n",
      "Epoch :  500  loss: 0.000327\n",
      "Epoch :  600  loss: 0.000242\n",
      "Epoch :  700  loss: 0.000186\n",
      "Epoch :  800  loss: 0.000148\n",
      "Epoch :  900  loss: 0.000120\n",
      "Epoch : 1000  loss: 0.000100\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    hidden = torch.zeros([1*1, batch_size, n_hidden])\n",
    "    output, _ = model(input_batch, hidden, output_batch) # (n_step, n_class)\n",
    "    \n",
    "    # target_batch: (1, n_step, n_class)\n",
    "    loss = criterion(output, target_batch.squeeze(0))\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(\"Epoch : {:4d}  loss: {:.6f}\".format(epoch + 1, loss))\n",
    "        \n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2d3eed42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ich mochte ein bier P -> i want a beer E\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "onehot_vector = np.eye(n_class)\n",
    "test_batch = [onehot_vector[[word_to_idx[word] for word in 'SPPPP']]]\n",
    "test_batch = torch.FloatTensor(test_batch)\n",
    "hidden = torch.zeros([1*1, batch_size, n_hidden])\n",
    "\n",
    "predicts, trained_attn = model(input_batch, hidden, test_batch)\n",
    "predicts = predicts.data.max(1, keepdim=True)[1]\n",
    "\n",
    "print(sentences[0], '->', \" \".join([idx_to_word[predict.item()] for predict in predicts.squeeze()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b8c8950f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show Attention\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.matshow(trained_attn, cmap='viridis')\n",
    "ax.set_xticks([0,1,2,3,4])\n",
    "ax.set_yticks([0,1,2,3,4])\n",
    "ax.set_xticklabels(sentences[0].split())\n",
    "ax.set_yticklabels(sentences[2].split())\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
